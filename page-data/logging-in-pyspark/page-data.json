{"componentChunkName":"component---src-templates-blog-post-js","path":"/logging-in-pyspark/","result":{"data":{"site":{"siteMetadata":{"title":"Shantanu's Weblog"}},"markdownRemark":{"id":"136d676a-8f3f-560c-a5de-a30a1f712d01","excerpt":"Logging while writing pyspark applications is a common issue. I’ve come across many questions on Stack overflow where beginner Spark programmers are worried…","html":"<p>Logging while writing pyspark applications is a common issue. I’ve come across many questions on Stack overflow where beginner Spark programmers are worried that they have tried logging using some means and it didn’t work.</p>\n<p>This short post will help you configure your pyspark applications with log4j. Know that this is only one of the many methods available to achieve our purpose. If you have a better way, you are more than welcome to share it via comments.</p>\n<p>For the sake of brevity, I will save the technical details and working of this method for another post.</p>\n<h3>Step 1:  Configure log4j properties</h3>\n<p>We will use something called as Appender. As per <a href=\"https://logging.apache.org/log4j/2.x/manual/appenders.html\">log4j documentation</a>, appenders are responsible for delivering LogEvents to their destination.</p>\n<p>Append the following lines to your log4j configuration properties. You’ll find the file inside your spark installation directory-</p>\n<p>Inside- <em>spark/conf/log4j.properties</em></p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\"># Define the root logger with Appender file\nlog4j.rootLogger=WARN, console, FILE\n\n# Define the file appender\nlog4j.appender.FILE=org.apache.log4j.DailyRollingFileAppender\n\n# Name of the log file\nlog4j.appender.FILE.File=/tmp/logfile.out\n\n# Set immediate flush to true\nlog4j.appender.FILE.ImmediateFlush=true\n\n# Set the threshold to DEBUG mode\nlog4j.appender.FILE.Threshold=debug\n\n# Set File append to true.\nlog4j.appender.FILE.Append=true\n\n# Set the Default Date pattern\nlog4j.appender.FILE.DatePattern=&#39;.&#39; yyyy-MM-dd\n\n# Default layout for the appender\nlog4j.appender.FILE.layout=org.apache.log4j.PatternLayout\nlog4j.appender.FILE.layout.conversionPattern=%m%n\n</code></pre></div>\n<p>You can refer to the log4j documentation to customise each of the property as per your convenience. However, this config should be just enough to get you started with basic logging.</p>\n<h3>Step 2: Use it in your Spark application</h3>\n<p>Inside your pyspark script, you need to initialize the logger to use log4j. The easy thing is, you already have it in your pyspark context!</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">sc <span class=\"token operator\">=</span> SparkContext<span class=\"token punctuation\">(</span>conf<span class=\"token operator\">=</span>conf<span class=\"token punctuation\">)</span>\nlog4jLogger <span class=\"token operator\">=</span> sc<span class=\"token punctuation\">.</span>_jvm<span class=\"token punctuation\">.</span>org<span class=\"token punctuation\">.</span>apache<span class=\"token punctuation\">.</span>log4j\nlog <span class=\"token operator\">=</span> log4jLogger<span class=\"token punctuation\">.</span>LogManager<span class=\"token punctuation\">.</span>getLogger<span class=\"token punctuation\">(</span>__name__<span class=\"token punctuation\">)</span>\nlog<span class=\"token punctuation\">.</span>warn<span class=\"token punctuation\">(</span><span class=\"token string\">\"Hello World!\"</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>Just save and quit! That’s it! Your spark script is ready to log to console and log file.</p>\n<p>I personally set the logger level to WARN and log messages inside my script as log.warn. Its not a good practice however if you set the log level to INFO, you’ll be inundated with log messages from Spark itself.</p>\n<p>Again, comments with better alternatives are welcome!</p>\n<p>Cheers!</p>","frontmatter":{"title":"Logging in PySpark","date":"July 04, 2016","description":null}},"previous":{"fields":{"slug":"/the-monster-known-as-vim/"},"frontmatter":{"title":"The Monster known as VIM"}},"next":{"fields":{"slug":"/the-halting-problem/"},"frontmatter":{"title":"The Halting Problem"}}},"pageContext":{"id":"136d676a-8f3f-560c-a5de-a30a1f712d01","previousPostId":"90accab9-af6c-5248-bb45-143770697206","nextPostId":"60b97b23-470f-59f8-9545-02963a5336dd"}},"staticQueryHashes":["2841359383"]}